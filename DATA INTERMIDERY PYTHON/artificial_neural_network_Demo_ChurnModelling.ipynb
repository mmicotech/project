{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Artificial Neural Network Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "## part 1- Data Preprocessing\n",
    "## part 2- Building ANN\n",
    "## part 3- Training ANN, Compiling ANN\n",
    "## part 4- Making predictions and evaluating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWZyYmS_UE_L"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.25.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.1.21)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.30.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.12.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\preeti\\anaconda3\\envs\\deeplearning\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#for using tensorflow for first time you need to install it\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxkJoQBkUIHC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZaTwK7ojXr2F",
    "outputId": "0b27a96d-d11a-43e8-ab4b-87c1f01896fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check tensorflow version\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1E0Q3aoKUCRX"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKWAkFVGUU0Z"
   },
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXUkhkMfU4wq"
   },
   "outputs": [],
   "source": [
    "#import dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "\n",
    "#take all the columns except last column(output) \n",
    "#leave first three columns(which dont have impact on the output) \n",
    "#store in X which is independent variable\n",
    "X = dataset.iloc[:, 3:-1].values   #start from column index 3 till one before last column\n",
    "\n",
    "'''[:, 3:-1]: The colon : before the comma represents all rows, \n",
    "and 3:-1 specifies columns from the 4th column (index 3)\n",
    "up to the second-to-last column (excluding the last column).'''\n",
    "\n",
    "\n",
    "#take the last column(ouptput) which is dependent variable\n",
    "y = dataset.iloc[:, -1].values   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "VYP9cQTWbzuI",
    "outputId": "797e7a64-9bac-436a-8c9c-94437e5e7587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "#have a look at independent variables \n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "38vKGE6Nb2RR",
    "outputId": "a815e42a-e0dd-4cb5-ab97-b17ead98fbc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#have a look at independent variable\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6bQ0UgSU-NJ"
   },
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "le5MJreAbW52"
   },
   "source": [
    "Label Encoding the \"Gender\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxVKWXxLbczC"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "#label encoding for gender column which is column with index 2\n",
    "X[:, 2] = le.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "-M1KboxFb6OO",
    "outputId": "e2b8c7e8-0cbc-4cdf-f4eb-7f0853a00b88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 0 ... 1 1 101348.88]\n",
      " [608 'Spain' 0 ... 0 1 112542.58]\n",
      " [502 'France' 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 0 ... 0 1 42085.58]\n",
      " [772 'Germany' 1 ... 1 0 92888.52]\n",
      " [792 'France' 0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUxGZezpbMcb"
   },
   "source": [
    "One Hot Encoding the \"Geography\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMXC8-KMVirw"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "\n",
    "'''\n",
    "The first transformer is specified as ('encoder', OneHotEncoder(), [1]):\n",
    "---'encoder': A custom name for this transformation step (you can choose any name).\n",
    "---OneHotEncoder(): The transformation method to be applied (one-hot encoding).\n",
    "---[1]: The index (or indices) of the column(s) to be transformed using this method.\n",
    "In this case, it’s column 1 (assuming 0-based indexing).\n",
    "\n",
    "remainder='passthrough':\n",
    "Indicates what to do with the columns not explicitly transformed.\n",
    "'passthrough' means that any columns not specified \n",
    "(other than the one at index 1) will be kept unchanged in the output.\n",
    "'''\n",
    "\n",
    "#execute the encoding\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "ZcxwEon-b8nV",
    "outputId": "23a98af4-5e33-4b26-c27b-f06e3c5d2baf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 ... 1 1 101348.88]\n",
      " [0.0 0.0 1.0 ... 0 1 112542.58]\n",
      " [1.0 0.0 0.0 ... 1 0 113931.57]\n",
      " ...\n",
      " [1.0 0.0 0.0 ... 0 1 42085.58]\n",
      " [0.0 1.0 0.0 ... 1 0 92888.52]\n",
      " [1.0 0.0 0.0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHol938cW8zd"
   },
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-TDt0Y_XEfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nParameters:\\nX: The feature matrix (input data).\\ny: The target vector (output labels).\\ntest_size: The proportion of data allocated for testing (e.g., 0.2 means 20% for testing).\\nrandom_state: Seed for the random number generator (ensures reproducibility).\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "'''\n",
    "Parameters:\n",
    "X: The feature matrix (input data).\n",
    "y: The target vector (output labels).\n",
    "test_size: The proportion of data allocated for testing (e.g., 0.2 means 20% for testing).\n",
    "random_state: Seed for the random number generator (ensures reproducibility).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RE_FcHyfV3TQ"
   },
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ViCrE00rV8Sk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Feature scaling ensures that all features are on a comparable scale \n",
    "and have similar ranges.\n",
    "\n",
    "Why Use Feature Scaling?\n",
    "\n",
    "Equal Contribution: Scaling guarantees each feature contributes equally in training. \n",
    "Without scaling, larger-scale features might dominate, \n",
    "leading to skewed results.\n",
    "\n",
    "Algorithm Performance: Many algorithms  perform better or converge faster\n",
    "when features are scaled.\n",
    "\n",
    "Numerical Stability: Avoiding significant scale differences between \n",
    "features prevents numerical instability issues during calculations.\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "#apply scaling on both training and testing data\n",
    "'''\n",
    "Two Steps Combined:\n",
    "\n",
    "Fit: Calculates the mean and standard deviation (or other parameters) \n",
    "from the training data.\n",
    "\n",
    "Transform: Applies the calculated parameters to standardize the data.\n",
    "\n",
    "so for X_train we apply fit_transform\n",
    "\n",
    "but for case of X_test we simply appy transform \n",
    "Applies the same scaling parameters (calculated during training) \n",
    "to the test data.\n",
    "Ensures consistency between training and testing data.\n",
    "'''\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zfEzkRVXIwF"
   },
   "source": [
    "## Part 2 - Building the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvdeScabXtlB"
   },
   "source": [
    "### Initializing the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dtrScHxXQox"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "What is a Sequential Model?\n",
    "A Sequential model is a linear stack of layers in deep learning.\n",
    "It’s the simplest type of neural network architecture.\n",
    "Layers are added sequentially, one after the other.\n",
    "'''\n",
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rP6urV6SX7kS"
   },
   "source": [
    "### Adding the input layer and the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bppGycBXYCQr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nann: Refers to the Sequential model (already initialized\\n\\nadd(): Method to add a layer to the model.\\n\\nDense: A fully connected layer (also known as a dense layer).\\n\\nunits=6: Specifies the number of neurons (units) in this layer. \\nIn this case, there are 6 neurons.\\n\\nactivation='relu': Specifies the activation function for these neurons. \\nHere, it’s the Rectified Linear Unit (ReLU).\\nReLU outputs the input directly if it’s positive.\\nOtherwise, it outputs zero.\\nMathematically: f(x)=max(0,x)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This line adds a hidden layer to the neural network model.\n",
    "#The layer has 6 neurons, and the ReLU activation function is applied \n",
    "#Hidden layers help the model learn complex patterns from the input data.\n",
    "\n",
    "\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
    "\n",
    "'''\n",
    "ann: Refers to the Sequential model (already initialized\n",
    "\n",
    "add(): Method to add a layer to the model.\n",
    "\n",
    "Dense: A fully connected layer (also known as a dense layer).\n",
    "\n",
    "units=6: Specifies the number of neurons (units) in this layer. \n",
    "In this case, there are 6 neurons.\n",
    "\n",
    "activation='relu': Specifies the activation function for these neurons. \n",
    "Here, it’s the Rectified Linear Unit (ReLU).\n",
    "ReLU outputs the input directly if it’s positive.\n",
    "Otherwise, it outputs zero.\n",
    "Mathematically: f(x)=max(0,x)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BELWAc_8YJze"
   },
   "source": [
    "### Adding the second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JneR0u0sYRTd"
   },
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OyNEe6RXYcU4"
   },
   "source": [
    "### Adding the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cn3x41RBYfvY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nann: Refers to the Sequential model (already initialized).\\n\\nadd(): Method to add a layer to the model.\\n\\nDense: A fully connected layer (also known as a dense layer).\\n\\nunits=1: Specifies the number of neurons (units) in this layer. \\nIn this case, there is only 1 neuron.\\nactivation='sigmoid': Specifies the activation function \\nfor this output layer. Here, it’s the sigmoid function.\\nA sigmoid function is a mathematical function \\nwith a characteristic “S”-shaped curve. \\nIt transforms any value in the domain (−∞,∞)  to a number between 0 and 1\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "'''\n",
    "ann: Refers to the Sequential model (already initialized).\n",
    "\n",
    "add(): Method to add a layer to the model.\n",
    "\n",
    "Dense: A fully connected layer (also known as a dense layer).\n",
    "\n",
    "units=1: Specifies the number of neurons (units) in this layer. \n",
    "In this case, there is only 1 neuron.\n",
    "activation='sigmoid': Specifies the activation function \n",
    "for this output layer. Here, it’s the sigmoid function.\n",
    "A sigmoid function is a mathematical function \n",
    "with a characteristic “S”-shaped curve. \n",
    "It transforms any value in the domain (−∞,∞)  to a number between 0 and 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JT4u2S1_Y4WG"
   },
   "source": [
    "## Part 3 - Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GWlJChhY_ZI"
   },
   "source": [
    "### Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fG3RrwDXZEaS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nann: Refers to the Sequential model (already initialized).\\n\\ncompile(): Method to configure the model for training.\\n\\noptimizer='adam': Specifies the optimization algorithm. \\nHere, we use the Adam optimizer, which adapts the learning rate during training.\\n\\nloss='binary_crossentropy': Defines the loss function. \\nIt’s a loss function used in binary classification problems.\\nSpecifically designed for models that predict probabilities.\\nMeasures the difference between predicted probabilities and actual class labels.\\n\\nmetrics=['accuracy']: Specifies evaluation metrics. \\nIn this case, we track accuracy during training.\\n\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This line sets up the model for training\n",
    "#by specifying how it should learn and evaluate itself.\n",
    "#The optimizer, loss function, and evaluation metric are crucial \n",
    "#for model performance.\n",
    "\n",
    "\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "'''\n",
    "ann: Refers to the Sequential model (already initialized).\n",
    "\n",
    "compile(): Method to configure the model for training.\n",
    "\n",
    "optimizer='adam': Specifies the optimization algorithm. \n",
    "Here, we use the Adam optimizer, which adapts the learning rate during training.\n",
    "\n",
    "loss='binary_crossentropy': Defines the loss function. \n",
    "It’s a loss function used in binary classification problems.\n",
    "Specifically designed for models that predict probabilities.\n",
    "Measures the difference between predicted probabilities and actual class labels.\n",
    "\n",
    "metrics=['accuracy']: Specifies evaluation metrics. \n",
    "In this case, we track accuracy during training.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QR_G5u7ZLSM"
   },
   "source": [
    "### Training the ANN on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nHZ-LKv_ZRb3",
    "outputId": "718cc4b0-b5aa-40f0-9b20-d3d31730a531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.5678 - accuracy: 0.7237\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4721 - accuracy: 0.7960\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4534 - accuracy: 0.7960\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4407 - accuracy: 0.7962\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4304 - accuracy: 0.8045\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4180 - accuracy: 0.8149\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4024 - accuracy: 0.8266\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3876 - accuracy: 0.8363\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3756 - accuracy: 0.8440\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3659 - accuracy: 0.8478\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3576 - accuracy: 0.8516\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3528 - accuracy: 0.8540\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3490 - accuracy: 0.8560\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3461 - accuracy: 0.8565\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3442 - accuracy: 0.8584\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3430 - accuracy: 0.8589\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3418 - accuracy: 0.8605\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3410 - accuracy: 0.8600\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3402 - accuracy: 0.8606\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3392 - accuracy: 0.8619\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3385 - accuracy: 0.8612\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3382 - accuracy: 0.8624\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8621\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3371 - accuracy: 0.8620\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3371 - accuracy: 0.8618\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8620\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3361 - accuracy: 0.8620\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3357 - accuracy: 0.8636\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8616\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 990us/step - loss: 0.3352 - accuracy: 0.8636\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3348 - accuracy: 0.8635\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3345 - accuracy: 0.8636\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 0s 996us/step - loss: 0.3344 - accuracy: 0.8626\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3344 - accuracy: 0.8630\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3340 - accuracy: 0.8626\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3340 - accuracy: 0.8620\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3340 - accuracy: 0.8639\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8644\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3336 - accuracy: 0.8614\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8635\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 0s 985us/step - loss: 0.3333 - accuracy: 0.8631\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 997us/step - loss: 0.3333 - accuracy: 0.8630\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3331 - accuracy: 0.8626\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3332 - accuracy: 0.8633\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3325 - accuracy: 0.8639\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3330 - accuracy: 0.8633\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3327 - accuracy: 0.8634\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3325 - accuracy: 0.8630\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3326 - accuracy: 0.8633\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3325 - accuracy: 0.8641\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3324 - accuracy: 0.8637\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3327 - accuracy: 0.8626\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3329 - accuracy: 0.8618\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3322 - accuracy: 0.8630\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 0s 991us/step - loss: 0.3327 - accuracy: 0.8627\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3323 - accuracy: 0.8635\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3321 - accuracy: 0.8634\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3320 - accuracy: 0.8625\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3321 - accuracy: 0.8643\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3319 - accuracy: 0.8635\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3320 - accuracy: 0.8620\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.8645\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3318 - accuracy: 0.8621\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3314 - accuracy: 0.8636\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.8635\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3317 - accuracy: 0.8644\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3314 - accuracy: 0.8644\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.8629\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8635\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3314 - accuracy: 0.8630\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3314 - accuracy: 0.8633\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.8633\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3313 - accuracy: 0.8641\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3311 - accuracy: 0.8620\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8636\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.8646\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8636\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8636\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8640\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3314 - accuracy: 0.8629\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3313 - accuracy: 0.8640\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3311 - accuracy: 0.8641\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3314 - accuracy: 0.8640\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3309 - accuracy: 0.8641\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3313 - accuracy: 0.8635\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3306 - accuracy: 0.8649\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3310 - accuracy: 0.8631\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3314 - accuracy: 0.8637\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3308 - accuracy: 0.8641\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3310 - accuracy: 0.8622\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3311 - accuracy: 0.8635\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3308 - accuracy: 0.8633\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3310 - accuracy: 0.8631\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3310 - accuracy: 0.8643\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3309 - accuracy: 0.8634\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3309 - accuracy: 0.8629\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3306 - accuracy: 0.8637\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3306 - accuracy: 0.8637\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3309 - accuracy: 0.8645\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3307 - accuracy: 0.8627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nann: Refers to the neural network model (already defined and compiled).\\n\\nfit(): Method to train the model using the specified training data.\\n\\nX_train: The feature matrix (input data) for training.\\n\\ny_train: The corresponding target labels (output values) for training.\\n\\nbatch_size=32: Specifies the number of samples in each batch during training \\n(helps with memory efficiency and convergence).\\n\\nepochs=100: The number of times the entire dataset is passed through \\nthe model during training.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This line trains the neural network model using the provided training data.\n",
    "#The model learns from the features (X_train) \n",
    "#and adjusts its weights to minimize the specified loss function \n",
    "#over the given number of epochs.\n",
    "\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)\n",
    "\n",
    "\n",
    "'''\n",
    "ann: Refers to the neural network model (already defined and compiled).\n",
    "\n",
    "fit(): Method to train the model using the specified training data.\n",
    "\n",
    "X_train: The feature matrix (input data) for training.\n",
    "\n",
    "y_train: The corresponding target labels (output values) for training.\n",
    "\n",
    "batch_size=32: Specifies the number of samples in each batch during training \n",
    "(helps with memory efficiency and convergence).\n",
    "\n",
    "epochs=100: The number of times the entire dataset is passed through \n",
    "the model during training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJj5k2MxZga3"
   },
   "source": [
    "## Part 4 - Making the predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84QFoqGYeXHL"
   },
   "source": [
    "### Predicting the result of a single observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGRo3eacgDdC"
   },
   "source": [
    "**Homework**\n",
    "\n",
    "Use our ANN model to predict if the customer with the following informations will leave the bank: \n",
    "\n",
    "Geography: France\n",
    "\n",
    "Credit Score: 600\n",
    "\n",
    "Gender: Male\n",
    "\n",
    "Age: 40 years old\n",
    "\n",
    "Tenure: 3 years\n",
    "\n",
    "Balance: \\$ 60000\n",
    "\n",
    "Number of Products: 2\n",
    "\n",
    "Does this customer have a credit card ? Yes\n",
    "\n",
    "Is this customer an Active Member: Yes\n",
    "\n",
    "Estimated Salary: \\$ 50000\n",
    "\n",
    "So, should we say goodbye to that customer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhU1LTgPg-kH"
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2d8IoCCkeWGL",
    "outputId": "957f3970-e197-4c3b-a150-7f69dc567f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step\n",
      "[[False]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAnd here we choose a threshold of 0.5\\n\\nto say that if that predicted probability\\n\\nis larger than 0.5,\\n\\nwell we will consider the final result to be one,\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The ann.predict method is used to obtain predictions \n",
    "#from a trained neural network model.\n",
    "\n",
    "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)\n",
    "\n",
    "'''\n",
    "And here we choose a threshold of 0.5\n",
    "\n",
    "to say that if that predicted probability\n",
    "\n",
    "is larger than 0.5,\n",
    "\n",
    "well we will consider the final result to be one,\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGjx94g2n7OV"
   },
   "source": [
    "Therefore, our ANN model predicts that this customer stays in the bank!\n",
    "\n",
    "**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array.\n",
    "\n",
    "**Important note 2:** Notice also that the \"France\" country was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the first row of the matrix of features X, \"France\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, because the dummy variables are always created in the first columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7yx47jPZt11"
   },
   "source": [
    "### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "nIyEeQdRZwgs",
    "outputId": "82330ba8-9bdc-4fd1-d3cf-b6d78ee7c2a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 849us/step\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnp.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)), 1):\\n\\nnp.concatenate: A NumPy function that combines arrays along a specified axis.\\n\\ny_pred and y_test are arrays containing predicted labels and true labels, \\nrespectively.\\n\\nreshape(len(y_pred), 1): Reshapes each array to have a single column \\n(vertical shape).\\n\\naxis=1: Specifies concatenation along columns (horizontally).\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "#The concatenated array shows predicted labels alongside true labels \n",
    "#for comparison.\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n",
    "\n",
    "'''\n",
    "np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)), 1):\n",
    "\n",
    "np.concatenate: A NumPy function that combines arrays along a specified axis.\n",
    "\n",
    "y_pred and y_test are arrays containing predicted labels and true labels, \n",
    "respectively.\n",
    "\n",
    "reshape(len(y_pred), 1): Reshapes each array to have a single column \n",
    "(vertical shape).\n",
    "\n",
    "axis=1: Specifies concatenation along columns (horizontally).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0oyfLWoaEGw"
   },
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ci6K_r6LaF6P",
    "outputId": "4d854e9e-22d5-432f-f6e5-a102fe3ae0bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1526   69]\n",
      " [ 198  207]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nA confusion matrix (also known as an error matrix) is a table \\nthat summarizes the performance of a machine learning model \\non a set of test data. It provides a detailed breakdown of the \\nnumber of accurate and inaccurate predictions made by the model.\\n\\nHere’s how it works:\\n\\nEach row of the matrix represents instances from an actual class (ground truth).\\n\\nEach column represents instances predicted by the model.\\n\\nThe diagonal of the matrix shows correctly predicted instances \\n(true positives and true negatives).\\n\\nOff-diagonal elements represent misclassifications \\n(false positives and false negatives).\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "'''\n",
    "A confusion matrix (also known as an error matrix) is a table \n",
    "that summarizes the performance of a machine learning model \n",
    "on a set of test data. It provides a detailed breakdown of the \n",
    "number of accurate and inaccurate predictions made by the model.\n",
    "\n",
    "Here’s how it works:\n",
    "\n",
    "Each row of the matrix represents instances from an actual class (ground truth).\n",
    "\n",
    "Each column represents instances predicted by the model.\n",
    "\n",
    "The diagonal of the matrix shows correctly predicted instances \n",
    "(true positives and true negatives).\n",
    "\n",
    "Off-diagonal elements represent misclassifications \n",
    "(false positives and false negatives).\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>Predicted Positive</th>\n",
    "    <th>Predicted Negative</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Actual Positive</td>\n",
    "    <td>True Positive (TP)</td>\n",
    "    <td>False Negative (FN)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Actual Negative</td>\n",
    "    <td>False Positive (FP)</td>\n",
    "    <td>True Negative (TN)</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Accuracy:</strong></p>\n",
    "<p>Accuracy measures the proportion of correct predictions (both true positives and true negatives) out of all predictions made.</p>\n",
    "<p>It answers the question: “How often was the model correct?”</p>\n",
    "<p>Formula: \\( \\text{Accuracy} = \\frac{TP + TN}{TP + FN + FP + TN} \\)</p>\n",
    "\n",
    "<p><strong>Precision:</strong></p>\n",
    "<p>Precision focuses on the accuracy of positive predictions.</p>\n",
    "<p>It answers the question: “When the model predicted TRUE, how often was it right?”</p>\n",
    "<p>Precision is crucial when the cost of false positives (Type I errors) is high.</p>\n",
    "<p>Formula: \\( \\text{Precision} = \\frac{TP}{TP + FP} \\)</p>\n",
    "\n",
    "<p><strong>Recall (Sensitivity):</strong></p>\n",
    "<p>Recall measures the proportion of actual positive instances correctly identified by the model.</p>\n",
    "<p>It answers the question: “Out of all actual positive cases, how many did the model predict correctly?”</p>\n",
    "<p>Recall is important when minimizing false negatives (Type II errors) is critical.</p>\n",
    "<p>Formula: \\( \\text{Recall} = \\frac{TP}{TP + FN} \\)</p>\n",
    "\n",
    "<p>These metrics help us evaluate the performance of a classification model. Remember that the choice of metric depends on the specific problem and the associated costs of different types of errors.</p>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "artificial_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
